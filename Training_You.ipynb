{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training You",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyML0koNa/RFOmtE6ZkraayA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nuwandavek/you/blob/master/Training_You.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm5EzwNPr8w0"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTVbuOL6Xsx3",
        "outputId": "34e7ed58-632a-464c-c1e4-1f72fda9fa9e"
      },
      "source": [
        "!pip install transformers\n",
        "!git clone https://github.com/huggingface/transformers.git\n",
        "!pip install ./transformers\n",
        "!pip install -r ./transformers/examples/language-modeling/requirements.txt\n",
        "!mkdir output"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.5MB 12.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 50.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.9MB 24.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=6c4985f15db1c5fa2f270eb5415e26b9c82bb75e1d5087067e72fceb6acae0f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.1.1\n",
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 57740 (delta 2), reused 3 (delta 2), pack-reused 57729\u001b[K\n",
            "Receiving objects: 100% (57740/57740), 43.02 MiB | 28.91 MiB/s, done.\n",
            "Resolving deltas: 100% (40520/40520), done.\n",
            "Processing ./transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.0.dev0) (0.0.43)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.0.dev0) (3.0.12)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.0.dev0) (0.9.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.0.dev0) (0.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.0.dev0) (1.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.0.dev0) (20.8)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==4.2.0.dev0) (4.41.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.2.0.dev0) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.2.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.2.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.2.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.2.0.dev0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.2.0.dev0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.2.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==4.2.0.dev0) (2.4.7)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.2.0.dev0-cp36-none-any.whl size=1527266 sha256=20f2f8a1769325162d5eb86eac3a14964a4140b4110804d5092c56605ca24e75\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8biast5a/wheels/23/19/dd/2561a4e47240cf6b307729d58e56f8077dd0c698f5992216cf\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Found existing installation: transformers 4.1.1\n",
            "    Uninstalling transformers-4.1.1:\n",
            "      Successfully uninstalled transformers-4.1.1\n",
            "Successfully installed transformers-4.2.0.dev0\n",
            "Collecting datasets>=1.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/38/0c24dce24767386123d528d27109024220db0e7a04467b658d587695241a/datasets-1.1.3-py3-none-any.whl (153kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 163kB 22.4MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 16.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from -r ./transformers/examples/language-modeling/requirements.txt (line 3)) (3.12.4)\n",
            "Collecting pyarrow>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e1/27958a70848f8f7089bff8d6ebe42519daf01f976d28b481e1bfd52c8097/pyarrow-2.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.7MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.7MB 218kB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/73/826b19f3594756cb1c6c23d2fbd8ca6a77a9cd3b650c9dec5acc85004c38/xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 245kB 58.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets>=1.1.3->-r ./transformers/examples/language-modeling/requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets>=1.1.3->-r ./transformers/examples/language-modeling/requirements.txt (line 1)) (4.41.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets>=1.1.3->-r ./transformers/examples/language-modeling/requirements.txt (line 1)) (0.3.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets>=1.1.3->-r ./transformers/examples/language-modeling/requirements.txt (line 1)) (1.1.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets>=1.1.3->-r ./transformers/examples/language-modeling/requirements.txt (line 1)) (0.8)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets>=1.1.3->-r ./transformers/examples/language-modeling/requirements.txt (line 1)) (0.70.11.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets>=1.1.3->-r ./transformers/examples/language-modeling/requirements.txt (line 1)) (1.19.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->-r ./transformers/examples/language-modeling/requirements.txt (line 3)) (51.0.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->-r ./transformers/examples/language-modeling/requirements.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r ./transformers/examples/language-modeling/requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r ./transformers/examples/language-modeling/requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r ./transformers/examples/language-modeling/requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r ./transformers/examples/language-modeling/requirements.txt (line 1)) (2020.12.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets>=1.1.3->-r ./transformers/examples/language-modeling/requirements.txt (line 1)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets>=1.1.3->-r ./transformers/examples/language-modeling/requirements.txt (line 1)) (2.8.1)\n",
            "Installing collected packages: pyarrow, xxhash, datasets, sentencepiece\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "Successfully installed datasets-1.1.3 pyarrow-2.0.0 sentencepiece-0.1.94 xxhash-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb6HmORjsKtw"
      },
      "source": [
        "## Upload files for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vfrb4h88z_mf"
      },
      "source": [
        "import re\n",
        "\n",
        "def RemoveTimestamps(text):\n",
        "  return re.sub(b'\\d+/\\d+/\\d+.*-\\ ', b'', text)\n",
        "\n",
        "def UnicodeString(bytes_string):\n",
        "  return bytes_string.decode('utf-8')\n",
        "\n",
        "def AddSeparators(file_text):\n",
        "  return b'#\\n'.join(file_text.split(b'\\n'))\n",
        "\n",
        "CHUNK_LENGTH = 500\n",
        "def ChunkFile(file_text):\n",
        "  lines = file_text.split(b'\\n')\n",
        "  chunks = []\n",
        "  for line_index in range(0, len(lines), CHUNK_LENGTH):\n",
        "    chunk = b'\\n'.join(lines[line_index:line_index+CHUNK_LENGTH])\n",
        "    chunk += b'<|endoftext|>'\n",
        "    chunks.append(chunk)\n",
        "  return chunks\n",
        "\n",
        "from itertools import chain\n",
        "import random\n",
        "def MixChunks(chunked_files):\n",
        "  all_chunks = [chunk for chunked_file in chunked_files for chunk in chunked_file]\n",
        "  random.shuffle(all_chunks)\n",
        "  return all_chunks\n",
        "\n",
        "def ConvertChunksToString(chunks):\n",
        "  return b'\\n'.join(chunks)\n",
        "\n",
        "def GetShuffledAndCleanedTextFromFiles(file_contents):\n",
        "  file_chunks = []\n",
        "  for file_content in file_contents:\n",
        "    file_chunks.append(ChunkFile(AddSeparators(RemoveTimestamps(file_content))))\n",
        "  return ConvertChunksToString(MixChunks(file_chunks))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "ZJMvYY7cbw9W",
        "outputId": "8883852d-1423-4cfa-8fce-f6237407362f"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded_files = files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-614f50d5-5bf7-4471-a0da-55dba65a0a70\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-614f50d5-5bf7-4471-a0da-55dba65a0a70\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving WhatsApp Chat with 5 Years Time ðŸŒž.txt to WhatsApp Chat with 5 Years Time ðŸŒž.txt\n",
            "Saving WhatsApp Chat with Mihir London.txt to WhatsApp Chat with Mihir London.txt\n",
            "Saving WhatsApp Chat with Sreejith.txt to WhatsApp Chat with Sreejith.txt\n",
            "Saving WhatsApp Chat with Sreejith2.txt to WhatsApp Chat with Sreejith2.txt\n",
            "Saving WhatsApp Chat with Vikrant London.txt to WhatsApp Chat with Vikrant London.txt\n",
            "Saving WhatsApp Chat with Rishi Amreeka.txt to WhatsApp Chat with Rishi Amreeka.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDPYjntwb2US"
      },
      "source": [
        "cleaned_text = GetShuffledAndCleanedTextFromFiles(uploaded_files.values())\n",
        "data_file = open('data.txt', 'wb')\n",
        "data_file.write(cleaned_text)\n",
        "data_file.close()\n",
        "num_lines = cleaned_text.count(b'\\n')\n",
        "test_size = int(0.1 * num_lines)\n",
        "train_size = num_lines - test_size\n",
        "data_file.close()\n",
        "!tail -n {test_size} data.txt > test.txt\n",
        "!head -n {train_size} data.txt > train.txt"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUTUpZtaHsZL"
      },
      "source": [
        "import random\n",
        "\n",
        "def SampleTextFromFile(file):\n",
        "  file_contents = open(file).readlines()\n",
        "  begin = random.randint(0, len(file_contents) - 50)\n",
        "  for line in file_contents[begin:begin+50]:\n",
        "    print(line, end='')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejxPLIfHIMuy",
        "outputId": "1c86cef4-498d-4b45-8d69-1fdb3dc3d143"
      },
      "source": [
        "SampleTextFromFile('train.txt')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vivek: Can transfer after one hour of adding#\n",
            "Vivek: What verification thing?#\n",
            "Sreejith2: The bank account should have 40 lakhs thing#\n",
            "Sreejith2: Keep it and transfer after no?#\n",
            "Vivek: Yoyo all that is over \\m/#\n",
            "Sreejith2: Wooh!#\n",
            "Sreejith2: Peace peace#\n",
            "Vivek: That was required before visa#\n",
            "Sreejith2: Transfer off then#\n",
            "Vivek: Now peacemax#\n",
            "Vivek: ðŸ˜…#\n",
            "Vivek: Yoyoyo#\n",
            "Sreejith2: Hahaha nice nice!#\n",
            "Vivek: What plans today?#\n",
            "Vivek: Free for a call?#\n",
            "Sreejith2: Hey, no plans as such#\n",
            "Sreejith2: Yo in 5 mins#\n",
            "Vivek: Yoyo#\n",
            "Vivek: Ping me#\n",
            "Sreejith2: Haan#\n",
            "Vivek: Eyo#\n",
            "Vivek: I sent 1000 rs#\n",
            "Vivek: Got that?#\n",
            "Vivek: Once you confirm I'll transfer the rest#\n",
            "Sreejith2: Hey got#\n",
            "Sreejith2: Got 1k#\n",
            "Vivek: Yoyoyo#<|endoftext|>\n",
            "Himaya: hope I have a good day#\n",
            "Mihir London: https://player.vimeo.com/video/427943452#\n",
            "Mihir London: Wtf#\n",
            "Sreejith2: Wow that's amazing ðŸ¤¯#\n",
            "Rishi Amreeka: https://youtu.be/fZSFNUT6iY8#\n",
            "Rishi Amreeka: Have you guys seen this one?#\n",
            "Rishi Amreeka: Pretty insane#\n",
            "Sreejith2: Wow, wtf!#\n",
            "Sreejith2: Bet this would win pioneer easy ðŸ˜…#\n",
            "Vivek: Are we playing tomorrow?#\n",
            "Vivek: :)#\n",
            "Vivek: Wtfffff#\n",
            "Vikrant London: They also released an API yesterday https://beta.openai.com/#\n",
            "Vivek: Yeah, I saw this.#\n",
            "Vikrant London: I'm down ðŸ‘»#\n",
            "Rishi Amreeka: I won't be able to make it in the for the first 4 hours#\n",
            "Vivek: Whats the start time?#\n",
            "Vivek: Also I've to do some work in the morning. Shall we start at 11am ish? Like last week?#\n",
            "Vivek: Sreejith wakes up at that time too I think#\n",
            "Sreejith2: Works#\n",
            "Sreejith2: 11:30 pm indian time iiuc#\n",
            "Sreejith2: What time can you make it?#\n",
            "Vikrant London: I dreamt about you, and in my dream you said the same thing#\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQYhEC7-uRZ6"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8KUxgU6_mjl",
        "outputId": "639687b7-9586-4f33-a983-5124a985a64b"
      },
      "source": [
        "!python ./transformers/examples/language-modeling/run_clm.py --model_name_or_path distilgpt2 --train_file train.txt --validation_file test.txt --do_train --do_eval --output_dir ./output --per_gpu_train_batch_size 1 --per_gpu_eval_batch_size 1 --save_steps 800 --eval_steps 800 --logging_steps 800 --evaluation_strategy steps --overwrite_output_dir --block_size 256"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-01-04 07:31:30.298503: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "01/04/2021 07:31:32 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "01/04/2021 07:31:32 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=./output, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, model_parallel=False, evaluation_strategy=EvaluationStrategy.STEPS, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_steps=0, logging_dir=runs/Jan04_07-31-32_60e0f3253a8c, logging_first_step=False, logging_steps=800, save_steps=800, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=800, dataloader_num_workers=0, past_index=-1, run_name=./output, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fp16_backend=auto, sharded_ddp=False, label_smoothing_factor=0.0, adafactor=False)\n",
            "Downloading: 2.57kB [00:00, 3.21MB/s]       \n",
            "Using custom data configuration default\n",
            "Downloading and preparing dataset text/default-afc9f2b369f14147 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/text/default-afc9f2b369f14147/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab...\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-afc9f2b369f14147/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab. Subsequent calls will reuse this data.\n",
            "01/04/2021 07:31:33 - INFO - filelock -   Lock 139886831465752 acquired on /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631.lock\n",
            "[INFO|file_utils.py:1301] 2021-01-04 07:31:33,099 >> https://huggingface.co/distilgpt2/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp5yt6esck\n",
            "Downloading: 100% 762/762 [00:00<00:00, 1.02MB/s]\n",
            "[INFO|file_utils.py:1305] 2021-01-04 07:31:33,115 >> storing https://huggingface.co/distilgpt2/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
            "[INFO|file_utils.py:1308] 2021-01-04 07:31:33,115 >> creating metadata file for /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
            "01/04/2021 07:31:33 - INFO - filelock -   Lock 139886831465752 released on /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631.lock\n",
            "[INFO|configuration_utils.py:431] 2021-01-04 07:31:33,115 >> loading configuration file https://huggingface.co/distilgpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
            "[INFO|configuration_utils.py:467] 2021-01-04 07:31:33,116 >> Model config GPT2Config {\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:431] 2021-01-04 07:31:33,133 >> loading configuration file https://huggingface.co/distilgpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
            "[INFO|configuration_utils.py:467] 2021-01-04 07:31:33,134 >> Model config GPT2Config {\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "01/04/2021 07:31:33 - INFO - filelock -   Lock 139883893078952 acquired on /root/.cache/huggingface/transformers/55051ac97dcc32f0a736d21a32a4d42b0d9b90f117ca7c38e65038b04bd5c3f5.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f.lock\n",
            "[INFO|file_utils.py:1301] 2021-01-04 07:31:33,157 >> https://huggingface.co/distilgpt2/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpua3l42r1\n",
            "Downloading: 100% 1.04M/1.04M [00:00<00:00, 35.8MB/s]\n",
            "[INFO|file_utils.py:1305] 2021-01-04 07:31:33,209 >> storing https://huggingface.co/distilgpt2/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/55051ac97dcc32f0a736d21a32a4d42b0d9b90f117ca7c38e65038b04bd5c3f5.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|file_utils.py:1308] 2021-01-04 07:31:33,209 >> creating metadata file for /root/.cache/huggingface/transformers/55051ac97dcc32f0a736d21a32a4d42b0d9b90f117ca7c38e65038b04bd5c3f5.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "01/04/2021 07:31:33 - INFO - filelock -   Lock 139883893078952 released on /root/.cache/huggingface/transformers/55051ac97dcc32f0a736d21a32a4d42b0d9b90f117ca7c38e65038b04bd5c3f5.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f.lock\n",
            "01/04/2021 07:31:33 - INFO - filelock -   Lock 139883893078952 acquired on /root/.cache/huggingface/transformers/9dfb299b74cdf7601ba7cd3a8073dbdac351caec0ed7ab5849b098b3c8ae3d57.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n",
            "[INFO|file_utils.py:1301] 2021-01-04 07:31:33,230 >> https://huggingface.co/distilgpt2/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpteyb_8yd\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 21.9MB/s]\n",
            "[INFO|file_utils.py:1305] 2021-01-04 07:31:33,272 >> storing https://huggingface.co/distilgpt2/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/9dfb299b74cdf7601ba7cd3a8073dbdac351caec0ed7ab5849b098b3c8ae3d57.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|file_utils.py:1308] 2021-01-04 07:31:33,272 >> creating metadata file for /root/.cache/huggingface/transformers/9dfb299b74cdf7601ba7cd3a8073dbdac351caec0ed7ab5849b098b3c8ae3d57.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "01/04/2021 07:31:33 - INFO - filelock -   Lock 139883893078952 released on /root/.cache/huggingface/transformers/9dfb299b74cdf7601ba7cd3a8073dbdac351caec0ed7ab5849b098b3c8ae3d57.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n",
            "01/04/2021 07:31:33 - INFO - filelock -   Lock 139883893340928 acquired on /root/.cache/huggingface/transformers/accb287b5a5396b2597382916b6cc939fdab1366e89475a92338d3971b3d02b7.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0.lock\n",
            "[INFO|file_utils.py:1301] 2021-01-04 07:31:33,301 >> https://huggingface.co/distilgpt2/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpkc9gspy1\n",
            "Downloading: 100% 1.36M/1.36M [00:00<00:00, 45.7MB/s]\n",
            "[INFO|file_utils.py:1305] 2021-01-04 07:31:33,357 >> storing https://huggingface.co/distilgpt2/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/accb287b5a5396b2597382916b6cc939fdab1366e89475a92338d3971b3d02b7.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|file_utils.py:1308] 2021-01-04 07:31:33,357 >> creating metadata file for /root/.cache/huggingface/transformers/accb287b5a5396b2597382916b6cc939fdab1366e89475a92338d3971b3d02b7.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "01/04/2021 07:31:33 - INFO - filelock -   Lock 139883893340928 released on /root/.cache/huggingface/transformers/accb287b5a5396b2597382916b6cc939fdab1366e89475a92338d3971b3d02b7.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0.lock\n",
            "[INFO|tokenization_utils_base.py:1802] 2021-01-04 07:31:33,357 >> loading file https://huggingface.co/distilgpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/55051ac97dcc32f0a736d21a32a4d42b0d9b90f117ca7c38e65038b04bd5c3f5.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1802] 2021-01-04 07:31:33,357 >> loading file https://huggingface.co/distilgpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/9dfb299b74cdf7601ba7cd3a8073dbdac351caec0ed7ab5849b098b3c8ae3d57.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1802] 2021-01-04 07:31:33,358 >> loading file https://huggingface.co/distilgpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/accb287b5a5396b2597382916b6cc939fdab1366e89475a92338d3971b3d02b7.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "01/04/2021 07:31:33 - INFO - filelock -   Lock 139883904736504 acquired on /root/.cache/huggingface/transformers/43a212e83e76bcb07f45be584cf100676bdbbbe9c13f9e5c1c050049143a832f.a83d881ec4d624fd4b5826dd026e315246c48c67504ff91c0500570e291a54ba.lock\n",
            "[INFO|file_utils.py:1301] 2021-01-04 07:31:33,437 >> https://huggingface.co/distilgpt2/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpjlc9m2lx\n",
            "Downloading: 100% 353M/353M [00:04<00:00, 82.9MB/s]\n",
            "[INFO|file_utils.py:1305] 2021-01-04 07:31:37,801 >> storing https://huggingface.co/distilgpt2/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/43a212e83e76bcb07f45be584cf100676bdbbbe9c13f9e5c1c050049143a832f.a83d881ec4d624fd4b5826dd026e315246c48c67504ff91c0500570e291a54ba\n",
            "[INFO|file_utils.py:1308] 2021-01-04 07:31:37,801 >> creating metadata file for /root/.cache/huggingface/transformers/43a212e83e76bcb07f45be584cf100676bdbbbe9c13f9e5c1c050049143a832f.a83d881ec4d624fd4b5826dd026e315246c48c67504ff91c0500570e291a54ba\n",
            "01/04/2021 07:31:37 - INFO - filelock -   Lock 139883904736504 released on /root/.cache/huggingface/transformers/43a212e83e76bcb07f45be584cf100676bdbbbe9c13f9e5c1c050049143a832f.a83d881ec4d624fd4b5826dd026e315246c48c67504ff91c0500570e291a54ba.lock\n",
            "[INFO|modeling_utils.py:1024] 2021-01-04 07:31:37,802 >> loading weights file https://huggingface.co/distilgpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/43a212e83e76bcb07f45be584cf100676bdbbbe9c13f9e5c1c050049143a832f.a83d881ec4d624fd4b5826dd026e315246c48c67504ff91c0500570e291a54ba\n",
            "[INFO|modeling_utils.py:1140] 2021-01-04 07:31:41,327 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1149] 2021-01-04 07:31:41,327 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "100% 31/31 [00:00<00:00, 37.56ba/s]\n",
            "100% 4/4 [00:00<00:00, 41.05ba/s]\n",
            "100% 31/31 [00:01<00:00, 19.72ba/s]\n",
            "100% 4/4 [00:00<00:00, 24.44ba/s]\n",
            "[INFO|trainer.py:395] 2021-01-04 07:31:58,344 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:395] 2021-01-04 07:31:58,345 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[WARNING|training_args.py:450] 2021-01-04 07:31:58,345 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:450] 2021-01-04 07:31:58,347 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[INFO|trainer.py:718] 2021-01-04 07:31:58,347 >> ***** Running training *****\n",
            "[INFO|trainer.py:719] 2021-01-04 07:31:58,347 >>   Num examples = 1487\n",
            "[INFO|trainer.py:720] 2021-01-04 07:31:58,347 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:721] 2021-01-04 07:31:58,347 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:722] 2021-01-04 07:31:58,347 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:723] 2021-01-04 07:31:58,347 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:724] 2021-01-04 07:31:58,347 >>   Total optimization steps = 4461\n",
            "[WARNING|training_args.py:450] 2021-01-04 07:31:58,351 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:467] 2021-01-04 07:31:58,351 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "{'loss': 2.688307800292969, 'learning_rate': 4.1033400582828964e-05, 'epoch': 0.5379959650302623}\n",
            " 18% 800/4461 [01:02<04:51, 12.58it/s][WARNING|training_args.py:467] 2021-01-04 07:33:00,759 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:1440] 2021-01-04 07:33:00,760 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1441] 2021-01-04 07:33:00,760 >>   Num examples = 162\n",
            "[INFO|trainer.py:1442] 2021-01-04 07:33:00,760 >>   Batch size = 1\n",
            "\n",
            "  0% 0/162 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 8/162 [00:00<00:02, 76.52it/s]\u001b[A\n",
            "  9% 14/162 [00:00<00:02, 70.22it/s]\u001b[A\n",
            " 13% 21/162 [00:00<00:02, 67.41it/s]\u001b[A\n",
            " 17% 28/162 [00:00<00:02, 65.21it/s]\u001b[A\n",
            " 21% 34/162 [00:00<00:02, 63.06it/s]\u001b[A\n",
            " 25% 41/162 [00:00<00:01, 62.71it/s]\u001b[A\n",
            " 29% 47/162 [00:00<00:01, 61.86it/s]\u001b[A\n",
            " 33% 54/162 [00:00<00:01, 61.41it/s]\u001b[A\n",
            " 37% 60/162 [00:00<00:01, 60.81it/s]\u001b[A\n",
            " 41% 67/162 [00:01<00:01, 61.41it/s]\u001b[A\n",
            " 46% 74/162 [00:01<00:01, 61.27it/s]\u001b[A\n",
            " 50% 81/162 [00:01<00:01, 61.47it/s]\u001b[A\n",
            " 54% 88/162 [00:01<00:01, 61.48it/s]\u001b[A\n",
            " 59% 95/162 [00:01<00:01, 60.90it/s]\u001b[A\n",
            " 63% 102/162 [00:01<00:00, 61.22it/s]\u001b[A\n",
            " 67% 109/162 [00:01<00:00, 59.82it/s]\u001b[A\n",
            " 72% 116/162 [00:01<00:00, 60.10it/s]\u001b[A\n",
            " 75% 122/162 [00:01<00:00, 59.99it/s]\u001b[A\n",
            " 79% 128/162 [00:02<00:00, 59.92it/s]\u001b[A\n",
            " 83% 135/162 [00:02<00:00, 60.48it/s]\u001b[A\n",
            " 88% 142/162 [00:02<00:00, 60.70it/s]\u001b[A\n",
            " 92% 149/162 [00:02<00:00, 60.94it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 2.510533094406128, 'eval_runtime': 2.6685, 'eval_samples_per_second': 60.708, 'epoch': 0.5379959650302623}\n",
            " 18% 800/4461 [01:05<04:51, 12.58it/s]\n",
            "100% 162/162 [00:02<00:00, 60.56it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:1248] 2021-01-04 07:33:03,429 >> Saving model checkpoint to ./output/checkpoint-800\n",
            "[INFO|configuration_utils.py:289] 2021-01-04 07:33:03,430 >> Configuration saved in ./output/checkpoint-800/config.json\n",
            "[INFO|modeling_utils.py:814] 2021-01-04 07:33:04,463 >> Model weights saved in ./output/checkpoint-800/pytorch_model.bin\n",
            "{'loss': 2.537803192138672, 'learning_rate': 3.2066801165657925e-05, 'epoch': 1.0759919300605245}\n",
            " 36% 1600/4461 [02:13<03:47, 12.55it/s][WARNING|training_args.py:467] 2021-01-04 07:34:11,850 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:1440] 2021-01-04 07:34:11,850 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1441] 2021-01-04 07:34:11,850 >>   Num examples = 162\n",
            "[INFO|trainer.py:1442] 2021-01-04 07:34:11,850 >>   Batch size = 1\n",
            "\n",
            "  0% 0/162 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 8/162 [00:00<00:02, 70.33it/s]\u001b[A\n",
            "  9% 15/162 [00:00<00:02, 68.01it/s]\u001b[A\n",
            " 14% 22/162 [00:00<00:02, 65.46it/s]\u001b[A\n",
            " 17% 28/162 [00:00<00:02, 63.43it/s]\u001b[A\n",
            " 22% 35/162 [00:00<00:02, 62.56it/s]\u001b[A\n",
            " 25% 41/162 [00:00<00:01, 61.23it/s]\u001b[A\n",
            " 29% 47/162 [00:00<00:01, 60.80it/s]\u001b[A\n",
            " 33% 53/162 [00:00<00:01, 60.26it/s]\u001b[A\n",
            " 37% 60/162 [00:00<00:01, 60.41it/s]\u001b[A\n",
            " 41% 67/162 [00:01<00:01, 60.42it/s]\u001b[A\n",
            " 46% 74/162 [00:01<00:01, 60.45it/s]\u001b[A\n",
            " 49% 80/162 [00:01<00:01, 59.34it/s]\u001b[A\n",
            " 54% 87/162 [00:01<00:01, 60.91it/s]\u001b[A\n",
            " 58% 94/162 [00:01<00:01, 60.50it/s]\u001b[A\n",
            " 62% 101/162 [00:01<00:01, 59.98it/s]\u001b[A\n",
            " 67% 108/162 [00:01<00:00, 60.13it/s]\u001b[A\n",
            " 70% 114/162 [00:01<00:00, 59.01it/s]\u001b[A\n",
            " 74% 120/162 [00:01<00:00, 58.90it/s]\u001b[A\n",
            " 78% 126/162 [00:02<00:00, 59.14it/s]\u001b[A\n",
            " 82% 133/162 [00:02<00:00, 59.45it/s]\u001b[A\n",
            " 86% 139/162 [00:02<00:00, 59.46it/s]\u001b[A\n",
            " 90% 146/162 [00:02<00:00, 60.09it/s]\u001b[A\n",
            " 94% 153/162 [00:02<00:00, 59.90it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.4566686153411865, 'eval_runtime': 2.7026, 'eval_samples_per_second': 59.943, 'epoch': 1.0759919300605245}\n",
            " 36% 1600/4461 [02:16<03:47, 12.55it/s]\n",
            "100% 162/162 [00:02<00:00, 59.71it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:1248] 2021-01-04 07:34:14,554 >> Saving model checkpoint to ./output/checkpoint-1600\n",
            "[INFO|configuration_utils.py:289] 2021-01-04 07:34:14,555 >> Configuration saved in ./output/checkpoint-1600/config.json\n",
            "[INFO|modeling_utils.py:814] 2021-01-04 07:34:15,473 >> Model weights saved in ./output/checkpoint-1600/pytorch_model.bin\n",
            "{'loss': 2.3512088012695314, 'learning_rate': 2.3100201748486887e-05, 'epoch': 1.6139878950907867}\n",
            " 54% 2400/4461 [03:24<02:45, 12.42it/s][WARNING|training_args.py:467] 2021-01-04 07:35:22,895 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:1440] 2021-01-04 07:35:22,895 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1441] 2021-01-04 07:35:22,895 >>   Num examples = 162\n",
            "[INFO|trainer.py:1442] 2021-01-04 07:35:22,895 >>   Batch size = 1\n",
            "\n",
            "  0% 0/162 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 8/162 [00:00<00:02, 72.81it/s]\u001b[A\n",
            "  9% 14/162 [00:00<00:02, 67.63it/s]\u001b[A\n",
            " 13% 21/162 [00:00<00:02, 65.57it/s]\u001b[A\n",
            " 17% 27/162 [00:00<00:02, 62.22it/s]\u001b[A\n",
            " 21% 34/162 [00:00<00:02, 62.18it/s]\u001b[A\n",
            " 25% 40/162 [00:00<00:01, 61.40it/s]\u001b[A\n",
            " 28% 46/162 [00:00<00:01, 60.51it/s]\u001b[A\n",
            " 32% 52/162 [00:00<00:01, 59.96it/s]\u001b[A\n",
            " 36% 58/162 [00:00<00:01, 59.28it/s]\u001b[A\n",
            " 40% 65/162 [00:01<00:01, 59.60it/s]\u001b[A\n",
            " 44% 72/162 [00:01<00:01, 60.11it/s]\u001b[A\n",
            " 48% 78/162 [00:01<00:01, 60.07it/s]\u001b[A\n",
            " 52% 84/162 [00:01<00:01, 59.83it/s]\u001b[A\n",
            " 56% 90/162 [00:01<00:01, 59.01it/s]\u001b[A\n",
            " 60% 97/162 [00:01<00:01, 60.02it/s]\u001b[A\n",
            " 64% 103/162 [00:01<00:00, 59.63it/s]\u001b[A\n",
            " 67% 109/162 [00:01<00:00, 59.22it/s]\u001b[A\n",
            " 71% 115/162 [00:01<00:00, 58.28it/s]\u001b[A\n",
            " 75% 121/162 [00:02<00:00, 58.29it/s]\u001b[A\n",
            " 79% 128/162 [00:02<00:00, 59.12it/s]\u001b[A\n",
            " 83% 135/162 [00:02<00:00, 59.49it/s]\u001b[A\n",
            " 88% 142/162 [00:02<00:00, 59.72it/s]\u001b[A\n",
            " 91% 148/162 [00:02<00:00, 59.77it/s]\u001b[A\n",
            " 95% 154/162 [00:02<00:00, 59.76it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.435103178024292, 'eval_runtime': 2.7192, 'eval_samples_per_second': 59.576, 'epoch': 1.6139878950907867}\n",
            " 54% 2400/4461 [03:27<02:45, 12.42it/s]\n",
            "100% 162/162 [00:02<00:00, 60.01it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:1248] 2021-01-04 07:35:25,615 >> Saving model checkpoint to ./output/checkpoint-2400\n",
            "[INFO|configuration_utils.py:289] 2021-01-04 07:35:25,616 >> Configuration saved in ./output/checkpoint-2400/config.json\n",
            "[INFO|modeling_utils.py:814] 2021-01-04 07:35:26,713 >> Model weights saved in ./output/checkpoint-2400/pytorch_model.bin\n",
            "{'loss': 2.310181884765625, 'learning_rate': 1.4133602331315848e-05, 'epoch': 2.151983860121049}\n",
            " 72% 3200/4461 [04:35<01:41, 12.47it/s][WARNING|training_args.py:467] 2021-01-04 07:36:33,877 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:1440] 2021-01-04 07:36:33,877 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1441] 2021-01-04 07:36:33,877 >>   Num examples = 162\n",
            "[INFO|trainer.py:1442] 2021-01-04 07:36:33,877 >>   Batch size = 1\n",
            "\n",
            "  0% 0/162 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 8/162 [00:00<00:02, 71.90it/s]\u001b[A\n",
            "  9% 14/162 [00:00<00:02, 67.46it/s]\u001b[A\n",
            " 13% 21/162 [00:00<00:02, 64.84it/s]\u001b[A\n",
            " 17% 27/162 [00:00<00:02, 62.04it/s]\u001b[A\n",
            " 20% 33/162 [00:00<00:02, 60.88it/s]\u001b[A\n",
            " 24% 39/162 [00:00<00:02, 60.60it/s]\u001b[A\n",
            " 28% 45/162 [00:00<00:01, 59.08it/s]\u001b[A\n",
            " 31% 51/162 [00:00<00:01, 59.32it/s]\u001b[A\n",
            " 35% 57/162 [00:00<00:01, 58.98it/s]\u001b[A\n",
            " 40% 64/162 [00:01<00:01, 59.10it/s]\u001b[A\n",
            " 44% 71/162 [00:01<00:01, 59.61it/s]\u001b[A\n",
            " 48% 77/162 [00:01<00:01, 59.06it/s]\u001b[A\n",
            " 51% 83/162 [00:01<00:01, 58.81it/s]\u001b[A\n",
            " 55% 89/162 [00:01<00:01, 58.68it/s]\u001b[A\n",
            " 59% 96/162 [00:01<00:01, 58.74it/s]\u001b[A\n",
            " 63% 102/162 [00:01<00:01, 58.56it/s]\u001b[A\n",
            " 67% 108/162 [00:01<00:00, 57.91it/s]\u001b[A\n",
            " 71% 115/162 [00:01<00:00, 59.05it/s]\u001b[A\n",
            " 75% 122/162 [00:02<00:00, 59.38it/s]\u001b[A\n",
            " 79% 128/162 [00:02<00:00, 58.86it/s]\u001b[A\n",
            " 83% 134/162 [00:02<00:00, 58.80it/s]\u001b[A\n",
            " 86% 140/162 [00:02<00:00, 58.16it/s]\u001b[A\n",
            " 90% 146/162 [00:02<00:00, 58.56it/s]\u001b[A\n",
            " 94% 152/162 [00:02<00:00, 58.72it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.4298512935638428, 'eval_runtime': 2.7525, 'eval_samples_per_second': 58.856, 'epoch': 2.151983860121049}\n",
            " 72% 3200/4461 [04:38<01:41, 12.47it/s]\n",
            "100% 162/162 [00:02<00:00, 58.86it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:1248] 2021-01-04 07:36:36,631 >> Saving model checkpoint to ./output/checkpoint-3200\n",
            "[INFO|configuration_utils.py:289] 2021-01-04 07:36:36,632 >> Configuration saved in ./output/checkpoint-3200/config.json\n",
            "[INFO|modeling_utils.py:814] 2021-01-04 07:36:37,617 >> Model weights saved in ./output/checkpoint-3200/pytorch_model.bin\n",
            "{'loss': 2.2172616577148436, 'learning_rate': 5.1670029141448105e-06, 'epoch': 2.6899798251513114}\n",
            " 90% 4000/4461 [05:46<00:37, 12.44it/s][WARNING|training_args.py:467] 2021-01-04 07:37:45,019 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:1440] 2021-01-04 07:37:45,019 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1441] 2021-01-04 07:37:45,019 >>   Num examples = 162\n",
            "[INFO|trainer.py:1442] 2021-01-04 07:37:45,019 >>   Batch size = 1\n",
            "\n",
            "  0% 0/162 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 8/162 [00:00<00:02, 73.39it/s]\u001b[A\n",
            "  9% 14/162 [00:00<00:02, 67.38it/s]\u001b[A\n",
            " 13% 21/162 [00:00<00:02, 65.58it/s]\u001b[A\n",
            " 17% 27/162 [00:00<00:02, 63.39it/s]\u001b[A\n",
            " 20% 33/162 [00:00<00:02, 62.08it/s]\u001b[A\n",
            " 24% 39/162 [00:00<00:02, 60.59it/s]\u001b[A\n",
            " 28% 45/162 [00:00<00:01, 59.62it/s]\u001b[A\n",
            " 31% 51/162 [00:00<00:01, 59.53it/s]\u001b[A\n",
            " 36% 58/162 [00:00<00:01, 59.79it/s]\u001b[A\n",
            " 40% 64/162 [00:01<00:01, 59.43it/s]\u001b[A\n",
            " 43% 70/162 [00:01<00:01, 59.56it/s]\u001b[A\n",
            " 47% 76/162 [00:01<00:01, 59.27it/s]\u001b[A\n",
            " 51% 82/162 [00:01<00:01, 59.43it/s]\u001b[A\n",
            " 55% 89/162 [00:01<00:01, 59.80it/s]\u001b[A\n",
            " 59% 96/162 [00:01<00:01, 59.97it/s]\u001b[A\n",
            " 64% 103/162 [00:01<00:00, 60.29it/s]\u001b[A\n",
            " 68% 110/162 [00:01<00:00, 59.13it/s]\u001b[A\n",
            " 72% 116/162 [00:01<00:00, 58.79it/s]\u001b[A\n",
            " 76% 123/162 [00:02<00:00, 59.34it/s]\u001b[A\n",
            " 80% 130/162 [00:02<00:00, 59.67it/s]\u001b[A\n",
            " 84% 136/162 [00:02<00:00, 59.56it/s]\u001b[A\n",
            " 88% 143/162 [00:02<00:00, 59.95it/s]\u001b[A\n",
            " 93% 150/162 [00:02<00:00, 60.35it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.4272449016571045, 'eval_runtime': 2.7187, 'eval_samples_per_second': 59.587, 'epoch': 2.6899798251513114}\n",
            " 90% 4000/4461 [05:49<00:37, 12.44it/s]\n",
            "100% 162/162 [00:02<00:00, 60.16it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:1248] 2021-01-04 07:37:47,739 >> Saving model checkpoint to ./output/checkpoint-4000\n",
            "[INFO|configuration_utils.py:289] 2021-01-04 07:37:47,740 >> Configuration saved in ./output/checkpoint-4000/config.json\n",
            "[INFO|modeling_utils.py:814] 2021-01-04 07:37:48,676 >> Model weights saved in ./output/checkpoint-4000/pytorch_model.bin\n",
            "100% 4460/4461 [06:30<00:00, 12.40it/s][INFO|trainer.py:878] 2021-01-04 07:38:28,564 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 390.2174, 'train_samples_per_second': 11.432, 'epoch': 3.0}\n",
            "100% 4461/4461 [06:30<00:00, 11.43it/s]\n",
            "[INFO|trainer.py:1248] 2021-01-04 07:38:28,583 >> Saving model checkpoint to ./output\n",
            "[INFO|configuration_utils.py:289] 2021-01-04 07:38:28,584 >> Configuration saved in ./output/config.json\n",
            "[INFO|modeling_utils.py:814] 2021-01-04 07:38:29,723 >> Model weights saved in ./output/pytorch_model.bin\n",
            "01/04/2021 07:38:29 - INFO - __main__ -   ***** Train results *****\n",
            "01/04/2021 07:38:29 - INFO - __main__ -     epoch = 3.0\n",
            "01/04/2021 07:38:29 - INFO - __main__ -     train_runtime = 390.2174\n",
            "01/04/2021 07:38:29 - INFO - __main__ -     train_samples_per_second = 11.432\n",
            "01/04/2021 07:38:29 - INFO - __main__ -   *** Evaluate ***\n",
            "[WARNING|training_args.py:467] 2021-01-04 07:38:29,799 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:1440] 2021-01-04 07:38:29,799 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1441] 2021-01-04 07:38:29,799 >>   Num examples = 162\n",
            "[INFO|trainer.py:1442] 2021-01-04 07:38:29,799 >>   Batch size = 1\n",
            "100% 162/162 [00:02<00:00, 59.27it/s]\n",
            "01/04/2021 07:38:32 - INFO - __main__ -   ***** Eval results *****\n",
            "01/04/2021 07:38:32 - INFO - __main__ -     perplexity = 11.295200694996275\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj_aJ6myvpZR"
      },
      "source": [
        "## Play with model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6riV2LC7vqPQ"
      },
      "source": [
        "from transformers import pipeline"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CV12Xz0wiI5"
      },
      "source": [
        "ft_generator = pipeline('text-generation', model='./output')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7oCnZNhPCEZ"
      },
      "source": [
        "def PrettyPrintPrediction(text):\n",
        "  print()\n",
        "  text = text.replace('#', '\\n')\n",
        "  print(text)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBnHXEjvf0HX"
      },
      "source": [
        "ft_generator( )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMfMQy-o2Q__",
        "outputId": "47c392ca-659d-4e17-d04f-1a0598780ba1"
      },
      "source": [
        "for text in ft_generator(\"Vivek: Mihir sucks :(#Sreejith2: I agree! Tell me more#Vivek: Dude he always makes fun of me#Vivek:\", max_length=256, num_return_sequences=3):\n",
        "  PrettyPrintPrediction(text['generated_text'])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Vivek: Mihir sucks \\m/\n",
            "Sreejith2: I agree! Tell me more\n",
            "Vivek: Dude he always makes fun of me\n",
            "Vivek: ðŸ¤£\n",
            "Sreejith2: Hey thanks man. There is just one guy in the building who says \"hey fuck you bitch\"\n",
            "Sreejith2: https://youtu.be/z6YQtJd8sq\n",
            "Vivek: He has more jokes than Hitler\n",
            "Vivek: ðŸ¤£\n",
            "Sreejith2: Oho\n",
            "Sreejith2: Hey, all peace is here man\n",
            "Vivek: Peace, will probably find peace here soon\n",
            "Sreejith2: I think only if you actually feel safe.\n",
            "Vivek: Yup.\n",
            "Sreejith2: Can stay in your car next morning\n",
            "Vivek: There?\n",
            "Sreejith2: Come to the police station\n",
            "Sreejith2: Wassup man\n",
            "Vivek: Hey!\n",
            "Sreejith2: Whose name you're working on?\n",
            "Vivek: I want to go to your place\n",
            "Sreejith2: What time\n",
            "\n",
            "Vivek: Mihir sucks \\m/\n",
            "Sreejith2: I agree! Tell me more\n",
            "Vivek: Dude he always makes fun of me\n",
            "Vivek: He's the same person\n",
            "Vivek: ðŸ™„\n",
            "Sreejith2: Hey how did you get your visa?\n",
            "Vivek: To the US?\n",
            "Sreejith2: Hahaha\n",
            "Vivek: I wanted to change the visa\n",
            "Vivek: I decided to apply for it\n",
            "Sreejith2: In Bangalore for the last 2 days\n",
            "Vivek: And you've come across this\n",
            "Vivek: ðŸ™ˆ\n",
            "Vivek: Also I'm going to go to London for the last 3 days\n",
            "Sreejith2: Hey y'all have you gotten the visa?\n",
            "Vivek: And you'll need to submit it to the new US mail\n",
            "Vivek: I have no visa\n",
            "Vivek: And there is no visa\n",
            "Sreejith2: Ah ok okayðŸ˜‹\n",
            "Vivek: ðŸ‘»\n",
            "Sreejith2: Also, don't you have one that will do it\n",
            "Sreejith2\n",
            "\n",
            "Vivek: Mihir sucks \\m/\n",
            "Sreejith2: I agree! Tell me more\n",
            "Vivek: Dude he always makes fun of me\n",
            "Vivek: Have you met?\n",
            "Sreejith2: Haan yeah\n",
            "Vivek: I'm meeting him tomorrow\n",
            "Sreejith2: <Media omitted>\n",
            "Sreejith2: <Media omitted>\n",
            "Vivek: He's being called\n",
            "Vivek: ðŸ˜‚ðŸ˜‚\n",
            "Vivek: Hahahahahahaha\n",
            "Vivek: ðŸ˜¬\n",
            "Sreejith2: I think he's a weird character\n",
            "Vivek: In SF and tech\n",
            "Sreejith2: Also\n",
            "Sreejith2: I don't know if she's in SF now\n",
            "Vivek: ðŸ™ˆ\n",
            "Sreejith2: Yo what location?\n",
            "Vivek: Eyo\n",
            "Sreejith2: Hey\n",
            "Sreejith2: Yoyo\n",
            "Vivek: There\n",
            "Vivek: You\n",
            "Vivek: Free for a call?\n",
            "Vivek: For a call?\n",
            "Sreejith2: Yo\n",
            "Vivek\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDRDEqH2vGMV"
      },
      "source": [
        "## Download model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fac6_VtIvHQ_",
        "outputId": "80f41b42-e58a-4228-cdde-a38945fe1b37"
      },
      "source": [
        "!zip model.zip ./output/*"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: output/checkpoint-1600/ (stored 0%)\n",
            "  adding: output/checkpoint-2400/ (stored 0%)\n",
            "  adding: output/checkpoint-3200/ (stored 0%)\n",
            "  adding: output/checkpoint-4000/ (stored 0%)\n",
            "  adding: output/checkpoint-800/ (stored 0%)\n",
            "  adding: output/config.json (deflated 51%)\n",
            "  adding: output/eval_results_clm.txt (stored 0%)\n",
            "  adding: output/merges.txt (deflated 53%)\n",
            "  adding: output/pytorch_model.bin (deflated 9%)\n",
            "  adding: output/special_tokens_map.json (deflated 52%)\n",
            "  adding: output/tokenizer_config.json (deflated 38%)\n",
            "  adding: output/trainer_state.json (deflated 70%)\n",
            "  adding: output/training_args.bin (deflated 46%)\n",
            "  adding: output/train_results.txt (deflated 10%)\n",
            "  adding: output/vocab.json (deflated 59%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Cvky9WHxLSd",
        "outputId": "c091e24f-bb09-4677-baa8-abcfb197dbcb"
      },
      "source": [
        "ls -l"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 302268\n",
            "-rw-r--r--  1 root root   1276333 Jan  4 07:30  data.txt\n",
            "-rw-r--r--  1 root root 305021348 Jan  4 07:58  model.zip\n",
            "drwxr-xr-x  7 root root      4096 Jan  4 07:38  \u001b[0m\u001b[01;34moutput\u001b[0m/\n",
            "drwxr-xr-x  3 root root      4096 Jan  4 07:31  \u001b[01;34mruns\u001b[0m/\n",
            "drwxr-xr-x  1 root root      4096 Dec 21 17:29  \u001b[01;34msample_data\u001b[0m/\n",
            "-rw-r--r--  1 root root    127482 Jan  4 07:30  test.txt\n",
            "-rw-r--r--  1 root root   1148810 Jan  4 07:30  train.txt\n",
            "drwxr-xr-x 15 root root      4096 Jan  4 07:28  \u001b[01;34mtransformers\u001b[0m/\n",
            "-rw-r--r--  1 root root    188024 Jan  4 07:29 'WhatsApp Chat with 5 Years Time ðŸŒž.txt'\n",
            "-rw-r--r--  1 root root     96072 Jan  4 07:29 'WhatsApp Chat with Mihir London.txt'\n",
            "-rw-r--r--  1 root root    493383 Jan  4 07:30 'WhatsApp Chat with Rishi Amreeka.txt'\n",
            "-rw-r--r--  1 root root    271150 Jan  4 07:29 'WhatsApp Chat with Sreejith2.txt'\n",
            "-rw-r--r--  1 root root    351486 Jan  4 07:29 'WhatsApp Chat with Sreejith.txt'\n",
            "-rw-r--r--  1 root root    509144 Jan  4 07:30 'WhatsApp Chat with Vikrant London.txt'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "IF1Qh8X6xrt1",
        "outputId": "f4c9ad0f-b020-47ff-8eee-0fe0ac1f8e1b"
      },
      "source": [
        "files.download('model.zip')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_8df03770-4b81-4eec-8bb7-342f78da7a4a\", \"model.zip\", 305021348)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWo3Az4WS04Y"
      },
      "source": [
        "## Things to do"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mECyf8hSS2Ab"
      },
      "source": [
        "* Mix up text chunks between test and train sets\n",
        "* Add separators between different people's statements\n",
        "* Understand how the training script works and make relevant modifications"
      ]
    }
  ]
}